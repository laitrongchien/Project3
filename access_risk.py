# -*- coding: utf-8 -*-
"""AccessRisk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1svQuswh6NZ2uTe8kaOLfV4LieTSlBx4s
"""

import pandas as pd
import random
import networkx as nx
import matplotlib.pyplot as plt
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Embedding, Flatten, Input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer

from google.colab import drive
drive.mount('/content/drive')

"""Read data common vunerabilities exposures"""

cves = pd.read_csv('/content/drive/MyDrive/Project3/cve.csv')
cves.head()

cves.isna().sum()

cves.dropna(how='any', inplace=True)
risk_probability = [cvss / 10 for cvss in cves['cvss']]
cves['risk_probability'] = risk_probability
cves['risk_probability'] = cves['risk_probability'].round(2)
cves.head(10)

cwes = pd.read_csv('/content/drive/MyDrive/Project3/cwes.csv')
cwes.head()

products = pd.read_csv('/content/drive/MyDrive/Project3/products.csv')
products.head()

products.dropna(how='any', inplace=True)
products.head()

# Get all CVEs for each product
cves_of_product = products.groupby('vulnerable_product')['cve_id'].agg(list).reset_index()
cves_of_product = cves_of_product.rename(columns={'cve_id': 'cve_list'})
# Print the CVEs
cves_of_product

merge_products = products.merge(cves, on='cve_id')
merge_products

group_data = merge_products.groupby('vulnerable_product').agg({
    'cve_id': list,
    'risk_probability': list
}).reset_index()
group_data

random_products = group_data.sample(n=2)
random_products

"""Create a graph from the dataset using NetworkX, example view graph"""

sample_graph = nx.Graph()
for _, row in random_products.iterrows():
    product = row['vulnerable_product']
    cve_list = row['cve_id']
    risk_list = row['risk_probability']
    sample_graph.add_node(product, node_type='Product')
    # Add nodes and edges for each CVE in the cve_list
    for cve, risk_prob in zip(cve_list, risk_list):
        sample_graph.add_node(cve, node_type='CVE')
        sample_graph.add_edge(product, cve, weight=risk_prob)

# Visualize the graph
# pos = nx.spring_layout(attack_graph)
# nx.draw(attack_graph, pos, with_labels=True, font_weight='bold')
# plt.show()
# Define a color map for node types
node_color_map = {'Product': 'blue', 'CVE': 'red'}

# Visualize the graph with customized parameters
pos = nx.spring_layout(sample_graph, seed=42)
node_color = [node_color_map[node_type] for node_type in nx.get_node_attributes(sample_graph, 'node_type').values()]
edge_color = 'gray'

plt.figure(figsize=(12, 8))
nx.draw(sample_graph, pos, with_labels=True, font_weight='bold', node_size=300, node_color=node_color, edge_color=edge_color, alpha=0.7)
edge_labels = nx.get_edge_attributes(sample_graph, 'weight')
nx.draw_networkx_edge_labels(sample_graph, pos, edge_labels=edge_labels)
plt.show()

attack_graphs = []

for _, row in group_data.iterrows():
    product = row['vulnerable_product']
    cve_list = row['cve_id']
    risk_list = row['risk_probability']
    attack_graph = nx.Graph()
    attack_graph.add_node(product, node_type='Product')
    # Add nodes and edges for each CVE in the cve_list
    for cve, risk_prob in zip(cve_list, risk_list):
        attack_graph.add_node(cve, node_type='CVE')
        attack_graph.add_edge(product, cve, weight=risk_prob)
    attack_graphs.append(attack_graph)
print(len(attack_graphs))

# risk_probability_value = cves[cves['cve_id'] == 'CVE-2016-0363']['risk_probability'].values[0]
# print(risk_probability_value)

"""Calculate risk probability of attack for each product"""

product_risk_level = {}

for attack_graph in attack_graphs:
  product_nodes = [node for node, data in attack_graph.nodes(data=True) if data.get('node_type') == 'Product']
  # print(product_nodes)

  for product_node in product_nodes:
    neighbors = attack_graph.neighbors(product_node)
    cve_nodes = [neighbor for neighbor in neighbors if attack_graph.nodes[neighbor]['node_type'] == 'CVE']
    # print(cve_nodes)
    risk_prob = [attack_graph[product_node][cve_node]['weight'] for cve_node in cve_nodes]
    # print(risk_prob)

    product_not_risk_probability = 1.0
    for prob in risk_prob:
      product_not_risk_probability *= (1 - prob)
    product_risk_probability = 1 - product_not_risk_probability

    product_risk_level[product_node] = product_risk_probability
# print(product_risk_level)

"""Label base on risk level"""

labels = []
for product_node, product_risk_prob in product_risk_level.items():
  if product_risk_prob < 0.4:
    labels.append('low')
  elif 0.4 <= product_risk_prob < 0.7:
    labels.append('medium')
  else:
    labels.append('high')
# print(labels)
group_data['risk_level'] = group_data['vulnerable_product'].map(dict(zip(product_risk_level.keys(), labels)))
group_data

# Convert risk levels to numerical labels
label_encoder = LabelEncoder()
group_data['risk_level_label'] = label_encoder.fit_transform(group_data['risk_level'])
group_data

X = group_data['cve_id']
X
y = group_data['risk_level_label']

"""Biểu diễn vulnerable_product_name bằng TF-IDF"""

# Biểu diễn vulnerable_product_name bằng TF-IDF
# vectorizer = TfidfVectorizer()
# vulnerable_product_names_tfidf = vectorizer.fit_transform(vulnerable_product_names)
# print(vulnerable_product_names_tfidf)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize CVEs and convert to sequences
from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences to have the same length
max_sequence_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_test_seq))
X_train_padded = keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=max_sequence_length)
X_test_padded = keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=max_sequence_length)

# Define the model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_sequence_length))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(3, activation='softmax'))  # Output layer with 3 units for 'low', 'medium', 'high' risk levels

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_padded, y_train, epochs=2, batch_size=32, validation_data=(X_test_padded, y_test))

loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f'Test Accuracy: {accuracy * 100:.2f}%')